\chapter{UAV Navigation Using the RBL Algorithm and LiDAR Sensing\label{chap:lidar}}

    \section{Introduction}
        \subsection{Motivation}
        \ac{UAV}s are increasingly being deployed in challenging, unstructured environments like dense forests, moving beyond their traditional use in open areas.
        This shift is driven by a growing demand for autonomous solutions in sectors like environmental monitoring, forestry management (health assessment \cite{kurovec_fel_clanek}), and search and rescue, where ground-based access is difficult. 
        Furthermore, this work aligns with the broader trend of deploying \ac{UAV}s for increasingly complex tasks, such as detailed infrastructure inspection (e.g., power lines, where autonomous navigation in cluttered, potentially high \ac{EMI} environments is essential (Fly4Future \cite{f4f_powerline_inspection})). 
        Enabling UAVs to reliably navigate point-to-point within these challenging settings is a foundational step towards realizing these advanced applications safely and efficiently.

        While the primary objective of this work is to enable successful point-to-point navigation within a forest using the \ac{RBL} algorithm, a significant secondary benefit emerges from this process. 
        By successfully navigating from point A to point B while simultaneously building a map of the traversed environment, the UAV generates valuable spatial data about the forest structure. 
        This autonomously generated map can then serve a vital purpose: enabling enhanced planning for future missions within the same area. 
        Once a map exists, subsequent UAV operations could potentially transition from purely reactive navigation strategies to more efficient, globally informed path planning algorithms such as TODO find some. 
        Leveraging prior knowledge of obstacle locations could significantly improve the safety and efficiency of future routine tasks.
        
        \subsection{Problem Statement}
            Operating UAVs effectively within complex, three-dimensional environments like forests presents significant navigational challenges that hinder widespread autonomous deployment. 
            The primary problems addressed in this work stem from:
            \begin{enumerate}
                \item \textbf{\ac{GNSS}-Denied Conditions: } \\
                Within forests, the dense canopy and other obstructions frequently block or scatter \ac{GNSS} signals, leading to unreliable or completely absent reception.
                This necessitates reliance on onboard sensors and algorithms (like \ac{SLAM} based on \ac{LiDAR} and \ac{IMU} data) for accurate localization and state estimation.
                \item \textbf{Cluttered and Unpredictable Environments: } \\
                Forests are inherently cluttered with numerous static obstacles (trees, trunks, branches) and potentially dynamic ones (animals, leafs, falling branches). 
                The navigation system must be capable of perceiving these obstacles in real-time and planning safe paths around them without prior knowledge of their exact layout.
            \end{enumerate}
            Therefore, the core problem is to develop and validate a robust autonomous navigation system that allows a UAV to reliably traverse between specified points in a cluttered, GNSS-denied forest environment using only onboard sensing and computation.

        \subsection{Objectives}
            The primary objectives within this chapter are: 
            \begin{itemize}
                \item \textbf{Integrate the \ac{RBL} with Onboard Sensing: } \\
                    Adapt the core of the \ac{RBL} to utilize real-time sensor data. 
                    This includes modification of sensing cell $\mathcal{S}$, because used \ac{LiDAR} doesnt see in all directions. 
                    Sensing cell needs to remain convex so the centroid computed from partitioned cell $\mathcal{A}$ is never computed outside of the set - for safety convergence towards the goal.
                \item \textbf{Process Point Cloud Data: } \\
                    Implement neccessary filtration to refine the raw point cloud data acquired from the \ac{LiDAR}.
                \item \textbf{Integrate external packages on the \ac{UAV}: } \\
                    Integrate and configure external software packages for simultaneous environmental mapping and robust state estimation, suitable for operation within the \ac{GNSS}-denied forest environment.
                \item \textbf{Experimental Validation: } \\
                    Conduct experiments to evaluate the performance, robustness, and effectiveness of the complete navigation solution. 
                    This validation will be performed through real-world flight tests in an actual forest.
            \end{itemize}

        \subsection{Chapter Overview}
            This chapter covers the \ac{LiDAR} perception process, covering data acquisition, preprocessing, mapping, and the method used to integrate mapped voxel data into the \ac{RBL} algorithm. 
            Subsequently, the practical challenges of implementing the system on a \ac{UAV} with sensor limitations are discussed, along with the software solutions employed. 
            The chapter proceeds to present the experimental validation, outlining the simulation setup and results, followed by the real-world forest experiment methodology, challenges, and performance analysis, illustrated with videos of successful flights \cite{aggressive_flight}, \cite{conservative_flight} and a mapping failure case \cite{flight_fail}. 
            Finally, a comparative analysis, discussion of limitations, and summary of key findings are presented.
    
    \section{LiDAR-Based Perception and Point Cloud Processing}
    \label{sec:lidar_perception}
        \subsection{Overview of LiDAR for UAV Navigation}
            \ac{LiDAR} is a crucial sensing technology widely used in applications such as \ac{SLAM} \cite{point_lio_paper}, autonomous vehicles \cite{Lidar_autonomous_vehicles}, UAVs TODOcite, and precision agriculture \cite{Lidar_agriculture}. 
            It provides high-resolution spatial data about the surrounding environment, making it a valuable tool for perception and navigation in dynamic and complex environments.  
            For UAV applications, \ac{LiDAR} serves several essential functions:  
            \begin{itemize}  
                \item \textbf{3D Mapping} -- Capturing a detailed representation of terrain, structures, and obstacles.  
                \item \textbf{Obstacle Detection} -- Identifying objects and estimating their position relative to the \ac{UAV} for collision avoidance.  
                \item \textbf{Autonomous Path Planning} -- Assisting navigation algorithms by providing spatial information for decision-making.  
                \item \textbf{Terrain Following} -- Helping the \ac{UAV} maintain a safe altitude by detecting variations in ground elevation.  
            \end{itemize}  

            \ac{LiDAR} offers several benefits that make it an attractive choice for \ac{UAV}-based navigation:  
            \begin{itemize}  
                \item \textbf{High Accuracy} -- Provides precise distance measurements, crucial for obstacle avoidance and localization.  
                \item \textbf{Environment Agnostic} -- Functions effectively in various conditions, including low-light environments and featureless terrain where cameras may fail.  
                \item \textbf{Fast Data Acquisition} -- Captures thousands to millions of points per second, enabling real-time processing.  
                \item \textbf{Rich Depth Information} -- Unlike cameras that provide only 2D images, \ac{LiDAR} generates accurate depth data, improving spatial awareness and 3D perception.  
            \end{itemize}  

            Despite its advantages, \ac{LiDAR} also presents certain challenges and limitations:  
            \begin{itemize}  
                \item \textbf{Computational Complexity} -- Processing large point clouds in real-time requires significant computational power, which may be a limitation for \ac{UAV}s with low processing resources.
                \item \textbf{Sensor Noise and Artifacts} -- External factors such as vibrations and motion of \ac{UAV} can introduce errors in point cloud data.  
                \item \textbf{Limited Field of View (FoV)} -- The placement of the \ac{LiDAR} sensor on the \ac{UAV} affects its coverage, requiring strategies to compensate for blind spots.  
                \item \textbf{Environmental Interference} -- Performance may degrade in challenging conditions such as fog, rain, or dense vegetation due light deviation.  
                \item \textbf{Power Consumption} -- \ac{LiDAR} sensors can consume a significant amount of power, which reduces the overall flight time of the \ac{UAV}.
                \item \textbf{Interference with Other LiDARs} -- \ac{LiDAR} sensors can experience interference when multiple units are used nearby, potentially leading to faulty measurements.
            \end{itemize}

        \subsection{Point Cloud Data Acquisition}
            \ac{LiDAR} systems determine object distances by emitting laser pulses and measuring the time it takes for the reflected light to return. 
            This process, known as \ac{ToF}, involves scanning the environment with laser beams directed at varying horizontal and vertical angles. 
            The reflected light, modulated in intensity, phase, or frequency, is captured by a receiver, which uses a lens to focus the signal onto a photodetector. 
            This detector converts the light into an electrical signal via the photoelectric effect \cite{lidar_how_works}.

            The system calculates distance based on the light's travel time, considering its near-light-speed propagation. 
            To distinguish transmitted from received signals, the laser's \setcounter{tocdepth}{1}
            wavelength is often adjusted. 
            Subsequent signal processing filters and analyzes the electrical signal, accounting for surface material and environmental variations. 
            The output is a 3D point cloud representing the scanned environment, along with reflected laser energy intensities. 
            All these data points are stored in a ROS message of type \(sensor\_msgs::PointCloud2\)

        \subsection{Preprocessing Techniques}
            To efficiently process \ac{LiDAR} data and reduce computational complexity, the raw point cloud undergoes downsampling and filtering. 
            The point cloud density is reduced using a voxel grid filter. 
            Subsequently, points associated with the \ac{UAV}'s structure are removed based on its known encumbrance.
            \begin{itemize}
                \item \textbf{Voxel Grid Downsampling} -- The raw \ac{LiDAR} point cloud often contains a large number of points, which can be computationally expensive to process in real-time. 
                To address this, we apply a voxel grid filter using the \ac{PCL} \cite{pcl_voxelgrid}. 
                This method partitions the 3D space into a grid of voxels with a given resolution (leafSize) and retains a single representative point per voxel. 
                The filtering process reduces the number of points while preserving the overall structure of the environment.
                \item \textbf{Filtering Points Corresponding to the UAV Structure} -- \ac{LiDAR} sensors mounted on \ac{UAV}s can capture unwanted points originating from the \ac{UAV} itself, such as reflections from its frame or rotor rods. 
                To prevent these points from interfering with navigation, additional filtering step has been applied.
                Points falling outside a specified distance range (closer than a minimum threshold or farther than a maximum threshold) are filtered out.
            \end{itemize}
            The resulting filtered point cloud contains only relevant environmental features while eliminating unnecessary points, improving efficiency for future processes.
            The point cloud is than fed into Point lio state estimator and bonxai mapping. 

        \subsection{Bonxai Mapping}
            As will be shown later in this chapter, prior terrain knowledge is beneficial for navigation. 
            For this purpose, environmental mapping was accomplished using a simple package developed by the \ac{MRS} group, implementing the Bonxai mapping approach.
            Specific details and a citation for this package are omitted as it represents an internal \ac{MRS} solution currently under development, however, various publicly available packages offer similar voxel-based mapping capabilities.
            The resulting map is represented as a voxel grid, which is then used by the \ac{RBL} algorithm for navigation planning.

            Initially, an approach involving surface reconstruction from this voxel data was investigated. 
            This involved estimating surface normals from the point cloud combined with Greedy Projection Triangulation (GP3) method from the PCL library to generate polygonal mesh approximating the environment's surface.
            While this method could produce relatively accurate surface representation, it turned out to be computationally expensive and too slow for real-time execution on the \ac{UAV}'s onboard computer.

            Due to these limitations, the surface reconstruction approach was abandoned. 
            Instead a simpler and more efficient method was adopted, which involves directly using the information about the environment stored in voxel grid map.
            This direct voxel usage approach is detailed in the subsequent section. 
            Consequently, further development of the mesh-based surface reconstruction method is not recommended for this application.

            \begin{figure}[htbp]
                \centering
                \includegraphics[width=0.48\textwidth]{./fig/rviz/triangulation_surface_aprox.png}
                \caption{
                    Surface approximation using Greedy Projection Triangulation.
                }
                \label{fig:triangulation}
            \end{figure}

        \subsection{Voxel-Based Modification of Cell $\mathcal{A}$}
            Given that the size of the voxels is known, this infromation can be used to refine the partitioning of cell $\mathcal{A}$ by treating voxels as obstacles.
            First, it is necessary to determine which voxels are relevant to the \ac{RBL} algorithm.
            This can be achieved by considering the scaling parameter $\eta$.
            A voxel is considered relevant if it lies within radius of $\frac{r_s}{\eta}$ of the agent.
            For each discrete point in  in the set composing cell $\mathcal{S}$, the nearest voxel is identified using k-d tree algorithm from the PCL library.
            Because the found point is the voxel's center, the closest pint within that voxel's boundaries is calculated to accurate partition the cell $\mathcal{A}$ from cell $\mathcal{S}$.

            Given a point $\mathbf{p_{\mathcal{S}}}$, the voxel center $\mathbf{v_c}$ and the voxel edge length $\mathbf{e}$, $\mathbf{p_{closest}}$, is computed as: 
            \begin{equation}
                \mathbf{p}_{closest} =
                \begin{pmatrix}
                    \text{clamp}(p{\mathcal{S},x}, v_{c,x} - \frac{\mathbf{e}}{2}, v_{c,x} + \frac{\mathbf{e}}{2}) \\
                    \text{clamp}(p_{\mathcal{S},y}, v_{c,y} - \frac{\mathbf{e}}{2}, v_{c,y} + \frac{\mathbf{e}}{2}) \\
                    \text{clamp}(p_{\mathcal{S},z}, v_{c,z} - \frac{\mathbf{e}}{2}, v_{c,z} + \frac{\mathbf{e}}{2})
                \end{pmatrix}\text{,}
            \end{equation}
            where function clamp(x, a, b) constrains the value x to the range [a, b].
            This constrains $\mathbf{p_{\mathcal{S}}}$ to the voxel boundaries.

            A point is excluded from the set if $ \| \mathbf{p_i} - \mathbf{p_{\mathcal{S}}} \| \leq \eta \cdot \| \mathbf{p_i} - \mathbf{p}_{closest}\|$.
            

    \section{Implementation and Integration on UAV}
    \label{sec:implementation_integration}
        \subsection{Challanges in Integration}
            The algorithm's performance is influenced by the limitations of the \ac{LiDAR} sensor used for environmental sensing. 
            While the algorithm functions optimally with a full 360° horizontal and 180° vertical \ac{FOV}, which would require multiple sensors, the experiments used a single Livox Mid 360 \ac{LiDAR} \cite{livox_mid360}. 
            This \ac{LiDAR} provides a 360° horizontal \ac{FOV} and only a 59° vertical \ac{FOV}, resulting in a sensing blind spot.

            To solve this limitation, several modification were implemented. 
            The \ac{LiDAR} was mounted at an angle $\gamma$, as shown in Figure \reffig{fig:uavs}, to enhance ground sensing and increase forward visibility.

            \begin{figure}[htbp]
                \centering
                \subfloat[UAV model with visualized \ac{LiDAR} mounting parameters.] {
                \includegraphics[width=0.48\textwidth]{./fig/photos/uav_side_view.png}
                \label{fig:model_uav}
                }
                \subfloat[UAV used in the experiment with mounted \ac{LiDAR}] {
                \includegraphics[width=0.48\textwidth]{./fig/photos/uav_photo.jpg}
                \label{fig:uav_1}
                }
                \caption{
                    UAV and \ac{LiDAR} mounting scheme.
                    Subfigure (a) shows a modeled UAV with visualized \ac{LiDAR} mounting parameters, including the \ac{LiDAR} elevation field of view $\alpha$ and tilt angle $\gamma$. 
                    Subfigure (b) displays the real UAV used in the experiment with the \ac{LiDAR} mounted according to the same configuration.                                                
                }
                \label{fig:uavs}
            \end{figure}

            In addition to the \ac{LiDAR} mounting angle, a software modification is implemented to account fo the \ac{LiDAR}'s limited \ac{FOV}.
            The algorithm uses map to partition cell $\mathcal{S}$ into cell $\mathcal{A}$, from which the centroid is computed to guide \ac{UAV} movement.
            To ensure safety, \ac{UAV} should only move only within their visible \ac{FOV}. 
            However, map information from areas outside the current \ac{FOV} can still be valuable. 

            Therefore, a constraint is intorduced - the \ac{UAV} maintains its yaw rotation towards the current centroid $\mathbf{c}_{\mathbf{A}}$ and moves towards it only if the centroid lies within a defined angular range in fron of the \ac{UAV}.

            To incorporate the \ac{LiDAR}'s \ac{FOV} into cell $\mathcal{S}$, two planes are defined based on the \ac{LiDAR}'s mounting configuration and vertical \ac{FOV}.
            Let $\mathbf{e}_z$ be unit vector alog the z-axis, $R_{off} \in SO(3)$ the offset rotation, and $R_{rpy} \in SO(3)$ the roll-pitch-yaw rotation.
            The normal vectors, $\mathbf{n}_1$ and $\mathbf{n}_2$ defining two planes are given by:
            \begin{equation}
                \mathbf{n}_1 = R_{rpy} \cdot R_{y}(\gamma) \cdot \mathbf{e}_z    
            \end{equation}
            \begin{equation}
                \mathbf{n}_2 = R_{rpy} \cdot R_{y}(\gamma - \alpha) \cdot \mathbf{e}_z  \text{,}
            \end{equation}
            where $\alpha$ is the \ac{LiDAR}'s \ac{FOV} and $\gamma$ is the the mounting angle of the \ac{LiDAR}.

            Cell $\mathcal{S}$ is then modified by excluding points that lie outside the \ac{LiDAR}'s \ac{FOV}, defined by these two planes: 

            \begin{equation}
                \mathcal{S}_i' = \{ \mathbf{q} \in \mathcal{S}_i \mid \mathbf{n}1 \cdot (\mathbf{q} - \mathbf{p}_{LiDAR}) \geq 0 \land \mathbf{n}2 \cdot (\mathbf{q} - \mathbf{p}_{LiDAR}) \leq 0 \}\text{,}
            \end{equation}
            where $\mathcal{S}_i'$ is the modified cell $\mathcal{S}$ and $\mathbf{p}_{LiDAR}$ is the exact position of the LiDAR sensor.

            This modification effectively restricts the points considered in the calculation of the centroid to only those within the \ac{LiDAR}'s \ac{FOV}.

            To incorporate map information from the surrounding area outside of the \ac{LiDAR}'s current \ac{FOV}, the following procedure is used.
            Firstly, both cells $\mathcal{S}_i$ and $\mathcal{S}_i'$ are created. These cells are partitioned into cells $\mathcal{A}_i$ and $\mathcal{A}_i'$ using voxels. 
            Cell $\mathcal{A}_i'$ is partitioned using voxels taht are actively sensed, while cell $\mathcal{A}_i$ is partitioned using the voxels from the whole surrounding.

            If the centroid computed from cell $\mathcal{A}_i$, $\mathbf{c}_{\mathcal{A}_i}$, is within the cell $\mathcal{A}_i'$, the \ac{UAV} is commanded to move towards $\mathbf{c}_{\mathcal{A}_i}$ while simu rotating its yaw towards it.
            However, if $\mathbf{c}_{\mathcal{A}_i}$ lies outside cell $\mathcal{A}_i'$, it is projected onto the closest point on the boundary of cell $\mathcal{A}_i'$.
            As the \ac{UAV} only moves if the centroid is within a certain angle in front of it, this projection onto the boundary of $\mathcal{A}_i'$ ensures that the \ac{UAV} primarily rotates towards the centroid until the centorid is within an angle in front of the \ac{UAV}.

            This procedure effectively utilizes the information fro mthe map while ensuring that the \ac{UAV} moves only within its actively sensed \ac{FOV}.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.48\textwidth]{./fig/rviz/cell_a_sliced_with_planes.png}
                \caption{
                    RViz visualization of LiDAR field-of-view (FOV) planes. The UAV is represented by the XYZ coordinate axes. 
                    The green squares represent the two planes, defined by normal vectors $\mathbf{n}_1$ and $\mathbf{n}_2$, used to slice cell $\mathcal{S}_i$ and determine the modified cell $\mathcal{S}_i'$.
                    The red sphere represents the centroid $\mathbf{c}_{\mathcal{A}_i}$.
                }
                \label{fig:cell_a_sliced}
            \end{figure}
    
    \section{Experimental Results}
    \label{sec:experimental_results}
        This section presents the findings from both simulated and real-world experiments conducted to evaluate the proposed approach.
        \subsection{Simulation Setup}
            In the simulations, a virtual forest environment was created. 
            Initially, a raycasting approach from the UAV to the simulated trees was considered for obstacle detection. 
            However, due to its computational cost, a static point cloud representation of the entire forest was generated and published instead. 
            The forest was modeled as a collection of trees, with trunks represented by cylinders and crowns by spheres.

            The forest was contained within a rectangular area defined by the following boundaries:
            \begin{itemize}
                \item $x_{\min}$ = -20.0 m
                \item $x_{\max}$ =  20.0 m
                \item $y_{\min}$ = -32.0 m
                \item $y_{\max}$ =  32.0 m
            \end{itemize}
            A total of 100 trees were randomly generated within this area, with a minimum separation distance of 3 meters between them. 
            The trees were generated with the following parameter variances:
            \begin{itemize}
                \item Trunk radius: 0.5 $\pm$ 0.2 m
                \item Tree height: 7.5 $\pm$ 2.0 m
                \item Crown radius: 2.5 $\pm$ 0.5 m
            \end{itemize}
            The UAV's starting position was set at one corner of the rectangular area (-22.0, -33.0, 2.0), and the goal position was set at the opposite corner (22.0, 33.0, 5.0), resulting in an approximate distance of 80 meters between them.

            It is important to note that the right-hand rule and z rule, described in the previous chapter, are not required for effective coordination in this static forest environment.
            Also, the point-lio estimator and Bonxai mapping were not simulated.

            The specific values of the parameters used in the experiments are summarized in following table \ref{tab:rbl_forest_simulation_parameters}:

            % \begin{table}[H]
            %     \centering
            %     \caption{Parameters Used in Experiments}
            %     \begin{tabular}{|l|l|l|l|l|l|l|l|}
            %         \hline
            %         % Row 1: Parameter names act as column headers here
            %         Parameter & $r_s$ [m]                  & Update rate [Hz] & $\delta_i$ [m]  & $d_1 = d_3 = d_5$  [m] & $d_2 = d_4 = d_6$ [m] & $\beta_i^D$ [] & $\eta$ []\\
            %         \hline \hline 
            %         Value     & 3.5                        & 10               & 0.5             & 0.5                    & 1.0                   & 0.5            & 0.9  \\
            %         \hline
            %     \end{tabular}
            %     \label{tab:rbl_forest_simulation_parameters_transposed} % Adjusted label
            % \end{table}

            \begin{table}[H]
                \centering
                \caption{Parameters Used in Experiments}
                \begin{tabular}{|l|c|}
                    \hline
                    Parameter & Value \\
                    \hline
                    \hline
                    Sensing radius $r_s$ [m] & 3.5 \\ \hline
                    Update rate [Hz] & 10  \\ \hline
                    Encumbrance $\delta_i$ [m] & 0.5  \\ \hline
                    $d_1 = d_3 = d_5$ [m] & 0.5  \\ \hline
                    $d_2 = d_4 = d_6$ [m] & 1.0  \\ \hline
                    $\beta_i^D$ [ ] & 0.5  \\ \hline
                    $\eta$ [ ] & 0.9  \\ \hline
                \end{tabular}
                \label{tab:rbl_forest_simulation_parameters}
            \end{table}
            
            The UAV dynamics are the same as described in the previous chapter and detailed in Table \ref{tab:uav_constraints}. 
            Each simulation was run 10 times.            

        \subsection{Performance in Simulated Environments}
            The simulation results highlight the proposed solution capability in navigating inside dense forest environments. 
            The UAV demonstrated effective obstacle avoidance, reacting appropriately to simulated trees while maintaining efficient trajectory without unnecessary detours.
            Convergence towards the designated goal waypoint was consistently achieved in a stable manner.

            \begin{table}[H]
                \centering
                \renewcommand{\arraystretch}{1.2}
                \begin{tabular}{|l|c|c|c|c|}
                \hline
                                                  & \( SR \ [\%] \) & \( \overline{L} \ [\mathrm{m}] \) & \( \overline{t} \ [\mathrm{s}] \) &  \( \overline{v} \ [\mathrm{m/s}] \)     \\ \hline
                Results                           & 100.00          & 91.99 $\pm$ 0.96                  & 150.89 $\pm$ 0.88                  &  0.85 $\pm$ 0.00                         \\ \hline
                \end{tabular}
            \end{table}

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.48\textwidth]{./fig/rviz/simulation_forest.png}
                \caption{
                    Visualization of simulated forest and the corresponding path of the UAV, indicated by a red line.
                }
                \label{fig:simulated_forest_path}
            \end{figure}

        \subsection{Real-World Experiments in a Forest Environment}
            Following the successful simulation validation, the \ac{RBL} algorithm was tested through real-world experiments conducted in a forest environment.
            \begin{itemize}
                \item \textbf{Location: } \\
                    The chosen forest site presented a relatively clear environment, considered 'friendly' for initial UAV flight tests. 
                    To increase the navigational challenge and better test the obstacle avoidance capabilities, additional branches were placed near trees within the flight area after a few successfull flights. 
                    The point cloud filtration parameters were set to discard points closer than 1.0 meter (to avoid detecting the \ac{UAV}'s own propellers) and farther than 40 meters.
                \item \textbf{UAV Platform: } \\
                    A custom-built multirotor UAV from the MRS group was used for this experiment (TODO ask for specifications). 
                    It was equipped with a primaryly with LiDAR mounted on top for environmental perception and state estimation.
                    While other sensors like GPS (unreliable in the forest), a Garmin altimeter, and a barometer were present on the platform, they were not directly used by the \ac{UAV}.
                \item \textbf{Experimental Procedure: } \\
                    Each experimental run followed a defined procedure. 
                    First, the necessary software components were launched within a tmux session on the ground. 
                    A safety pilot then armed the UAV and performed a manual takeoff to a safe altitude. 
                    Once airborne and stable, the \ac{RBL} algorithm was initiated, commanding the UAV towards a predefined goal. 
                    The flight was continuously monitored via visualization in Rviz and observing the \ac{UAV}. 
                    If the UAV approached an obstacle too closely or exhibited unsafe behavior, the safety pilot immediately terminated the autonomous mode, took manual control, and landed the UAV. 
                    Between runs, key algorithm parameters, such as those controlling navigation 'aggressivity' (e.g., the scale of cells in the RBL and the weighting function used for path planning), were adjusted based on observations from the previous flight, and the experiment was repeated.
                \item \textbf{Safety Measures: } \\
                    Safety was paramount throughout the experiments. 
                    A trained safety pilot maintained visual line-of-sight with the UAV at all times and was prepared to immediately take manual control via the remote controller should any unsafe condition arise.
                \item \textbf{Data Collected: } \\
                    For safety reasons, data logging was initiated only after the manual takeoff was complete. 
                    During the autonomous flight part and subsequent landing, all relevant ROS topics (including sensor data, estimated state, \ac{RBL} cells, centroid) were recorded into rosbag files for detailed post-flight analysis.
                \item \textbf{Challenges Encountered: } \\
                    Several challenges emerged during real-world deployment. 
                    A significant set-back involved correctly managing coordinate frame transformations, particularly due to the slight tilt of the top-mounted LiDAR sensor. 
                    Ensuring accurate alignment between the LiDAR's point cloud data, the Point-LIO state estimator, and the navigation algorithm's reference frame required careful configuration. 
                    Another challenge, which warrants further investigation (discussed in Future Work), related to the Bonxai mapping part. 
                    Occasionally, the mapper would map dynamic objects, such as leaves disturbed by propellers, into the static map. 
                    Due to the mapping algorithm's configuration, which was set to be conservative about removing potentially static obstacles to ensure safety, these incorrectly mapped 'ghost' obstacles were sometimes not removed even when subsequent scans showed they were no longer present. 
                    In some instances, this led to the UAV perceiving itself as surrounded by obstacles (effectively 'locking' itself), requiring the safety pilot to intervene and land.
            \end{itemize}

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.48\textwidth]{./fig/rviz/deadlock_moment_mapping.png}
                \caption{
                    Visualization of a moment where the UAV incorrectly mapped leaves as obstacles directly in its path.
                }
                \label{fig:map_fail}
            \end{figure}

        
        \subsection{Performance in Real-World Experiment}
            For the real world experiment I have chosen 2 flights to describe and one fail flight. For each of the flight I uploaded a video One conservative:
            \begin{table}[H]
                \centering
                \caption{Parameters Used in Experiments}
                \begin{tabular}{|l|c|c|}
                    \hline
                    Parameter & Values Conservative & Values Aggressive \\
                    \hline
                    \hline
                    Sensing radius ($r_s$) & 2.0 m & 2.0 m \\ \hline
                    Update rate & 10 Hz & 10 Hz \\ \hline
                    Encumbrance & 0.5 m & 0.5 m \\ \hline
                    $d_1 = d_3 = d_5$ & 0.5 m & 0.5 m \\ \hline
                    $d_2 = d_4 = d_6$ & 1.0 m & 1.0 m\\ \hline
                    $\beta_i^D$ & 0.5 & 0.1 \\ \hline
                    $\eta$ & 0.8 & 0.8 \\ \hline
                \end{tabular}
                \label{tab:rbl_forest_conservative_flight}
            \end{table}

            \begin{table}[H]
                \centering
                \renewcommand{\arraystretch}{1.2}
                \begin{tabular}{|l|c|c|c|c|}
                \hline
                                    & \( L \ [\mathrm{m}] \) & \( t \ [\mathrm{s}] \) &  \( v \ [\mathrm{m/s}] \)     \\ \hline
                Conservative Flight & 53.84                   & 116.40                  &  0.46                          \\ \hline
                Agressive Flight    & 35.56                   &  51.64                  &  0.69                          \\ \hline
                \end{tabular}
            \end{table}

            \begin{figure}[H]
                \centering
                \subfloat[] {
                \includegraphics[width=0.48\textwidth]{./fig/rviz/flight_484.png}
                }
                \subfloat[] {
                \includegraphics[width=0.48\textwidth]{./fig/rviz/flight_486.png}
                }
                \caption{
                    Visualization of the path the uav took during the flights a is conservative, b is agressinve.
                }
                \label{fig:rbl_forest_conservative_flight_path}
            \end{figure}



        \subsection{Comparative Analysis}
            The real-world flight experiments conducted in the forest environment successfully validated the navigation capabilities of the proposed setup, despite initial setbacks during setup related to odometry and mapping configuration. 
            The \ac{UAV} generally performed as expected based on simulation results, demonstrating its ability to navigate autonomously between points A and B while avoiding static obstacles.

            \subsection{Performance and Limitations}
                A primary limitation observed during real-world testing originated from the environmental mapping component, specifically the Bonxai mapping package used.
                While effective for static elements, the system occasionally struggled with dynamic obstacles, such as moving leaves.
                These obstacles were sometimes included into the voxel grid map and, due to the map's update policy (configured conservatively for safety), were not always removed immediately even when no longer present. 
                This could lead to situations where the \ac{UAV} perceived itself as blocked (as shown in the failure case video \cite{flight_fail}), requiring the safety pilot to land. 
                Addressing this mapping behavior is crucial for future testing.

            \subsection{Comparison with 2D Baseline}
                Direct comparison with other state-of-the-art 3D navigation methods is challenging due to the specific nature of the RBL-based approach. 
                However, a meaningful comparison can be done with the original 2D RBL implementation \cite{rbl_paper}. 
                The key differences lie in the sensing and dimensionality:
                \begin{itemize}
                    \item The 2D version relied on a 2D \ac{LiDAR} for planar obstacle detection and a separate optical distance measurement sensor to maintain a constant flight altitude. 
                    Remapping obstacles in 2D is often simpler.
                    \item 3D implementation utilizes a 3D \ac{LiDAR} for perception in three dimensions and must actively manage altitude based on the perceived environment, treating the ground itself as an obstacle.
                \end{itemize}
                A notable behavior observed in the 3D tests was the \ac{UAV}'s tendency to initially increase altitude after takeoff, effectively moving away from the ground obstacle, before potentially descending again as it neared the goal. 
                This behavior highlights the algorithm's capability to manage its vertical position based on the 3D environment.
                Illustrative examples of successful flights showcasing different parameter settings (aggressive vs. conservative) are provided in \cite{aggressive_flight} and \cite{conservative_flight}.

            \subsection{Future Work}
                The mapping limitations highlight key areas for future work. Potential solutions include:
                \begin{itemize}
                    \item \textbf{Dynamic Obstacle Handling: }\\
                    Implementing techniques to segment and filter out dynamic or transient objects from the map representation such as \cite{TRLO_good_mapping}.
                    \item \textbf{Reactive Navigation: }\\
                    Exploring strategies that rely more on direct sensor input for immediate obstacle avoidance, removing dependence on the map, though this introduces challenges in perceiving areas outside the current sensor view (e.g. behind the UAV).
                    \item \textbf{Map Update Logic: }\\
                    Investigating alternative mapping packages or implementing mechanisms like voxel decay instead of mapping (where unoccupied voxels gradually fade if not persistently observed) into the existing framework. 
                    While probabilistic mapping should ideally handle this, the developmental stage of the Bonxai package currently limit this capability.
                \end{itemize}
                Resolving these mapping challenges, particularly the robust handling of dynamic elements, is a prerequisite for advancing to more complex multi-agent experiments utilizing only onboard lidar sensing.
    
    \section{Conclusion}
    \label{sec:conclusion_lidar}
        This chapter detailed the implementation and real-world validation of the (\ac{RBL}) algorithm, adapted for autonomous \ac{UAV} navigation within a cluttered, \ac{GNSS}-denied forest environment using onboard 3D \ac{LiDAR} sensing. 
        The primary objective was to implement \ac{RBL} principles with real-time perception, state estimation (Point-LIO), and mapping (Bonxai) to achieve reliable point-to-point navigation.

        Key technical contributions included adapting the \ac{RBL} algorithm's cell partitioning to directly utilize voxelized map data created from processed \ac{LiDAR} point clouds. 
        Modifications were also introduced to effectively handle the practical limitations of a single \ac{LiDAR} sensor with a restricted vertical field of view, ensuring safe convergence by constraining movement based on actively sensed area while still leveraging map information. 
        An alternative approach using surface reconstruction via mesh generation was investigated but considered unsuitable due to computational complexity on the \ac{UAV} onboard computer.

        The proposed solution effectiveness was demonstrated through both simulation and real-world experiments conducted in a forest. 
        The \ac{UAV} successfully navigated between designated start and goal points, avoiding static obstacles like trees and adapting its altitude based on the \ac{LiDAR}'s perception. 

        Despite the overall success, the experiments identified a key limitation related to the Bonxai mapping package's handling of dynamic environmental elements, such as moving leaves, which occasionally led to navigation deadlocks. 
        This emphasises the importance of robust mapping with dynamic obstacles.

        In conclusion, this chapter successfully demonstrated the practical application and feasibility of using a 3D \ac{RBL} algorithm integrated with \ac{LiDAR} sensing for autonomous \ac{UAV} navigation in a challenging forest setting. 
        This capability can be observed in these videos \cite{aggressive_flight}, \cite{conservative_flight}.



